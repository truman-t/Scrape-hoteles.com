import time
import requests
from bs4 import BeautifulSoup as BS
from wolframclient.evaluation import WolframLanguageSession
from wolframclient.language import wl, wlexpr, Global
import pandas as pd
from datetime import datetime, date, timedelta
from random import uniform
import argparse

# The function get_html_hotelescom gets the html for a query to hoteles.com
def get_html_hotelescom(wolfram_session, web_browser, place, start_date, end_date, order='BEST_SELLER', rooms={1:{'adults':2}}):
    url = 'https://www.hoteles.com/search.do?$place$&q-check-in=$start_date$&q-check-out=$end_date$&$rooms$&sort-order=$order$'
    # Repalce the input of the function in the url
    url = url.replace('$place$', place)
    url = url.replace('$start_date$', start_date)
    url = url.replace('$end_date$', end_date)
    url = url.replace('$order$', order)
    # Replce rooms
    rooms_url = 'q-rooms='+str(len(rooms.keys()))
    i=0
    for a in rooms.values():
        rooms_url = rooms_url + '&' + 'q-room-' + str(i) + '-adults=' + str(a['adults'])
        i= i + 1
    rooms_url = rooms_url + '&' + '&'.join(['q-room-'+str(n)+'-children=0' for n in range(len(rooms.keys()))])
    url = url.replace('$rooms$', rooms_url)
    # Open the web page in a browser to charge dynamical content.
    # This will be donde using Wolfram (instead of more traditional packages like Selenium)
    wolfram_session.evaluate(wl.WebExecute(web_browser, wl.Rule("OpenPage", url))) # go to the url
    # scroll down using javascript code because some dynamical content is charged after scrolling down
    wolfram_session.evaluate(wl.WebExecute(web_browser, wl.Rule('JavascriptExecute', 'window.scrollTo(0,2000);')))
    time.sleep(uniform(3.8, 4.65))
    # get the html content using Javascript code
    html = wolfram_session.evaluate(wl.WebExecute(web_browser, wl.Rule('JavascriptExecute',\
                                                                          'return document.documentElement.outerHTML;')))
    return html

# The function get_data_hotelescom scrapes the html generated by get_html_hotelescom
def get_data_hotelescom(html):
    soup = BS(html)
    # Get the html element where is the information
    try:
        soup = soup.find_all('ul', {'class':'_3f26d2'})[0]
    except IndexError:
        return [[''], [''], [''], [''], [''], ['']]
    # Get the elements corresponding with hotels
    places = soup.findChildren(name = 'li', recursive=False)
    places = [element for element in places if 'data-hotel-id' in element.attrs.keys()]
    # Scrape the data
    links = []
    images = []
    names = []
    prices = []
    directions = []
    services = []
    for place in places:
        # Get data from soup
        # Get hotel name
        name = place.find('h2').getText()
        names.append(name)
        # Get hotel image
        images.append(place.find('img').get('src'))
        # Get link reservation
        links.append('www.hoteles.com'+place.find('a').get('href'))
        # Get data from text
        hotel_text = place.getText('\n\n').split('\n\n')
        # Get directions
        hotel_text = hotel_text[hotel_text.index(name)+1:]
        directions.append(next((direction for direction in hotel_text if \
                           (' estrella' not in direction) and ('VIP' not in direction)), ''))
        # Get services
        icon_positions = [hotel_text.index(symbol) for symbol in hotel_text if symbol.encode('unicode_escape').startswith(b'\\u')]
        service = '|'.join([hotel_text[i+1] for i in icon_positions])
        services.append(service)
        # Get price
        # This may depend in the currency that the page uses, which may depend on the IP adrres that the page reads.
        # In my case I use USD dolars, but if the page is used in other country this code should be changed.
        price = next((p for p in hotel_text if 'USD' in p), 'USD -1')
        price = float(price[4:].replace(',',""))
        prices.append(price)
    return [names, directions, images, services, links, prices]

# Top 10 tourist cities. Change this if you are interested in other cities.
cities = {'London, England, United Kingdom':'destination-id=549499', 'París, Francia':'destination-id=504261',
         'New York, New York, United States of America':'destination-id=1506246', 'Moscú, Rusia':'destination-id=1153093',
          'Dubai, Dubai, United Arab Emirates':'destination-id=11594', 'Tokio, Tokio (prefectura), Japón':'destination-id=726784',
          'Singapur, Singapur':'destination-id=1655844', 'Los Ángeles, California, Estados Unidos':'destination-id=1439028',
          'Barcelona, Barcelona, Cataluña, España':'destination-id=444495', 'Madrid, Comunidad de Madrid, España':'destination-id=457987'
         }
# Start Wolfran and Firefox sessions
session = WolframLanguageSession()
session.evaluate(wlexpr('browser = StartWebSession["Firefox"]'))
# Read the dates
parser = argparse.ArgumentParser()
parser.add_argument("--startDate", help="Enter start date of interval")
parser.add_argument("--endDate", help="Enter end date of interval")
args = parser.parse_args()
startDate = datetime.strptime(args.startDate, "%d/%m/%Y")
endDate = datetime.strptime(args.endDate,"%d/%m/%Y")
dates = [str((startDate+timedelta(days=x)).date()) for x in range((endDate-startDate).days)]
# Scrape
search_places = []
search_timestamps = []
search_dates = []
names = []
directions = []
images = []
services = []
links = []
prices = []
for day in dates:
    for (city, city_id) in cities.items():
        end_date = date.fromisoformat(day) + timedelta(days=1)
        end_date = str(end_date)
        timestamp = datetime.now().timestamp()
        # Scrape data
        html = get_html_hotelescom(session, Global.browser, city_id, day, end_date)
        (n, d, i, s, l, p) = get_data_hotelescom(html)
        # Append date in the lists
        names = names + n
        directions = directions + d
        images = images + i
        services = services + s
        links = links + l
        prices = prices + p
        search_places = search_places + [city]*len(n)
        search_timestamps = search_timestamps + [timestamp]*len(n)
        search_dates = search_dates + [day]*len(n)
df = pd.DataFrame({'cuidad_busqueda':search_places, 'fecha_busqueda':search_timestamps, 'fecha_registro':search_dates, 'nombre_establecimiento':names, 'direccion_establecimiento':directions,\
'servicios_establecimiento':services, 'imagen_establecimiento':images, 'link_reserva':links, 'precio':prices})
df.to_csv('testDataset.csv', index=False)
# Close sessions
session.evaluate(wl.DeleteObject(Global.browser))
session.terminate()
